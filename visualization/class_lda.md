While *k* means clustering can operate on 2 or more dimensions (features), it is an *unsupervised learning* method - meaning that it does not take into account the true class labels. If we have the true class labels, we may want to incoprorate that into our method and perfor *supervised learning*. In addition it is often useful to transform our data in such a way that we can capture more information from the data in a limited number of dimensions. This approach is referred to as a *dimension reduction* technique, which uses a *projection* of the data into a lower-dimensional space. A technique called *Linear Disciminant Analysis (LDA)* allows us to perform supervised classification as well as dimension reduction. LDA is closely related to *principal component analysis PCA*, in that we can visualize the data in reduced dimensions. A key difference is that PCA is an *unsupervised learning* method so it doesn't use the class labels and, thus, LDA is expected to separate the known classes better by design.

In our case, we have 3 classes, and we can represent the classifier in 2-dimensional space (in general, if we have K classes, we can represent the LDA classifier in K-1 dimensions). We won't go into the details of the technique, but want to emphasize the utility of LDA as a *supervised learning* approach, whereby the method uses the features (in our case, sepal and petal measurements) **and** the class labels (in our case, species) to build a classifier. This is in contrast to *k*-means clustering (above), which is an *unsupervised learning* method to group the data into clusters.

Once we have performed LDA on our data, we are left with 2 (in general K-1) *linear disciminants (LDs)*. In the plot below, we show the two linear discriminants - LD1 and LD2. Each disciminant explains a certain amount of the variance in the data, using a different (linear) combination of the features in each LD. Here, we see that LD1 explains 99% of the variance in the data. It is not too surprising that one variable explains 99% of the variance because we can see in the histograms of our *pairs* plot that `Petal.Width` and `Petal.Length` alone separate the classes fairly well.

Each dot in the plot below is a pair of coordinates given by LD1 and LD2 corresponding to each sample (iris). How about the colors? Using the LDA, these are the predicted class labels to classify the different dots into species, with each color corresponding to a different prediction. Note that in the *k*-means example, the colors were arbitrary and represented any clusters.  In this case, the colors are actually assigning a point to a species (green: *virginica*; red: *versicolor*; black: *setosa*). 

If we click on the `Indicate True Species` check box, we can start to see how well our LDA method did on this data. Clicking on the second checkbox (`Toggle All/Misclassified Irises`), will highlight the samples misclassified by LDA - surprisingly, only 3 samples were wrongly labeled! Importantly, these 3 samples lie very close to the decision boundary for the two species, and thus can be attributed to the overlap in features. Click on `Show LDA Decision Boundaries` to view the decision boundaries. This means that any points within the green shaded area would be classified by our model as belonging to *virginica*, red to *versicolor*, and black to *setosa*. Using LDA, we have managed to improve significantly upon our *k*-means clustering approach (with *k*=2), where we had about 10 misclassifications. Much of this is thanks to LDA taking advantage of the known class labels and using all 4 features.
