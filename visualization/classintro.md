*Clustering* is a means to find groups within a data set that are separable based on certain combinations of features. One of the most popular approaches in data analysis is *k*-means clustering, which aims to separate the data into *k* clusters, such that each observation belongs to the *k* cluster it is closest to. What is beautiful about *k*-means clustering is that we make no assumptions about the data (as it is a *non-parametric* method) - we simply look at what we have, and build our model up from there. Below, we show an interactive example, where we can define *k* clusters and see how our data is classified into these *k* clusters (and whether this agrees with the true species classification). 

In our example, the different colors correspond to the different *k* clusters, with the "X" showing the center (mean) of each *k* cluster.

* Try decreasing the cluster count from 3 to 2, and then increasing it up to 9. What do you think happens as we continue to increase *k*? What would happen if we set *k* equal to *n* (150 in this case)?
* Change the X and Y variables; do different combinations produce clearer separation between the *k* groups?
* Click the "Indicate True Species" checkbox. How would we evaluate  *k* means clustering performance on this data set?

We can see that as we change the number of clusters, we get different centroids ("X" marks) corresponding to the different means. As we increase *k*, we begin to see that some of the defined clusters begin to appear to be artifacts of the data - this phenomena is known as *overfitting*. If we were to set *k* equal to *n*, this would produce a completely different cluster per data point - not at all useful in terms of classifying our data!

Using different variables for X and Y, we can see the effect on the different groupings. When we set *k* equal to 3 (the number of species), what you'll notice immediately is that for any two variables, one of the groups is clearly different (at the bottom/bottom-left in the plots), while two of the groups seem to blend into each other. In this case, consider that if we were to add a little more data, the predicted class of this "boundary" samples between the two merged *k* clusters may change completely - these points at the edge are sensitive to new observations. Conversely, points near the "X"'s are likely impervious to new data from the same population.

Now, if we click on "Indicate True Species" (with *k* still set to 3) we can see how well our *k* means clustering performs in classifying the three different species. You'll notice that it is *versicolor* and *virginica* which can get confused by the algorithm, and that many of these confusing points indeed lie at the boundary of the between the two groups' *k* means. We can classify the accuracy of our clustering by looking at the *misclassification* rate, or the number of wrong labels that are assigned. In the case of the *versicolor*, with *k* of 3 and separating on `Sepal.Length` and `Petal.Width`, we see that 12/50 *virginicas* are misclassified as belonging to the predominantly *versicolor* group. We see similar misclassification rates using other combinations of features. As Fisher himself stated in his original paper, using only these features, it is impossible to *perfectly* classify the different species - but we will see that it is possible to substantially improve our method, and reduce the amount of misclassification. Let's take a look in the next section.


